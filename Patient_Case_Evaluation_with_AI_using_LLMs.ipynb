{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sLGHbV9fh1XG"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLzY/X7Twrd1o8s56zrUpb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65966824d56a4d85a3835e3dbe8a9a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff0ac02b204c480bbfc797198f92e8bc",
              "IPY_MODEL_f116a2cfe04248058ac010834dc65faf",
              "IPY_MODEL_c8cb3cd30896407283a28be6fb4e3524"
            ],
            "layout": "IPY_MODEL_67e74420732f418ab9306728c3ebcc00"
          }
        },
        "ff0ac02b204c480bbfc797198f92e8bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3baaf8f4c0314faebe9556d7e384d0b4",
            "placeholder": "​",
            "style": "IPY_MODEL_f537c1c96bf14c909237f510e3ccf2c4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f116a2cfe04248058ac010834dc65faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db73ea1b62944a9dafaedeb567dd94ad",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea5e7ee122264d1888caf3221caf4fa4",
            "value": 2
          }
        },
        "c8cb3cd30896407283a28be6fb4e3524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7293fffae894ab89a8cce475cb4a0b7",
            "placeholder": "​",
            "style": "IPY_MODEL_9e779f6d6b5742c0a6ede814b210887a",
            "value": " 2/2 [00:04&lt;00:00,  2.06s/it]"
          }
        },
        "67e74420732f418ab9306728c3ebcc00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3baaf8f4c0314faebe9556d7e384d0b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f537c1c96bf14c909237f510e3ccf2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db73ea1b62944a9dafaedeb567dd94ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea5e7ee122264d1888caf3221caf4fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7293fffae894ab89a8cce475cb4a0b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e779f6d6b5742c0a6ede814b210887a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gallifantjack/llm_teaching/blob/main/Patient_Case_Evaluation_with_AI_using_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Patient Case Evaluation with AI using LLMs\n"
      ],
      "metadata": {
        "id": "nmKq1MmywBGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Instructions\n",
        "\n",
        "### Step 1: Set up the Correct Runtime\n",
        "1. Go to `Runtime` --> `Change Runtime Type`\n",
        "2. Click `T4 GPU` *(you should then see T4 under Comment in the top right)*\n",
        "\n",
        "This step ensures that you have the necessary GPU acceleration for running the large language model efficiently.\n",
        "\n",
        "### Step 2: Run the Setup Cell\n",
        "Below this markdown, you'll find a collapsed code cell labeled \"Setup\". This cell contains essential functions for loading the data and the model. To run it:\n",
        "\n",
        "1. Click on the arrow to expand the cell.\n",
        "2. Run the cell by clicking the play button or pressing Shift+Enter.\n",
        "3. Wait for the cell to finish executing. This may take a few moments as it loads the necessary libraries and models."
      ],
      "metadata": {
        "id": "I3mpYx_blLWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate torch -q\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "import os\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_id = \"nvidia/Llama3-ChatQA-1.5-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# Load retriever model (optional, for longer documents)\n",
        "retriever_tokenizer = AutoTokenizer.from_pretrained('nvidia/dragon-multiturn-query-encoder')\n",
        "query_encoder = AutoModel.from_pretrained('nvidia/dragon-multiturn-query-encoder')\n",
        "context_encoder = AutoModel.from_pretrained('nvidia/dragon-multiturn-context-encoder')\n",
        "\n",
        "# Load the CSV file\n",
        "csv_url = \"https://raw.githubusercontent.com/AIM-Harvard/OncQA/main/Data/original_questions_gpt4_outputs/Master2.csv\"\n",
        "df = pd.read_csv(csv_url)\n",
        "\n",
        "def truncate_case_study(case_study):\n",
        "    split_text = case_study.split(\"Patient message:\")\n",
        "    return split_text[0].strip()\n",
        "\n",
        "def add_context_to_case_study(case_study, context, position=\"beginning\"):\n",
        "    if position.lower() == \"beginning\":\n",
        "        return f\"{context}\\n\\n{case_study}\"\n",
        "    elif position.lower() == \"end\":\n",
        "        return f\"{case_study}\\n\\n{context}\"\n",
        "    else:\n",
        "        raise ValueError(\"Position must be either 'beginning' or 'end'\")\n",
        "\n",
        "def get_formatted_input(messages, context):\n",
        "    system = \"System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\"\n",
        "    instruction = \"Please give a full and complete answer for the question.\"\n",
        "\n",
        "    for item in messages:\n",
        "        if item['role'] == \"user\":\n",
        "            item['content'] = instruction + \" \" + item['content']\n",
        "            break\n",
        "\n",
        "    conversation = '\\n\\n'.join([\"User: \" + item[\"content\"] if item[\"role\"] == \"user\" else \"Assistant: \" + item[\"content\"] for item in messages]) + \"\\n\\nAssistant:\"\n",
        "    formatted_input = system + \"\\n\\n\" + context + \"\\n\\n\" + conversation\n",
        "\n",
        "    return formatted_input\n",
        "\n",
        "def get_model_response(formatted_input):\n",
        "    tokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors=\"pt\").to(model.device)\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "    outputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n",
        "    response = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "def retrieve_relevant_chunks(query, chunks, top_n=5):\n",
        "    formatted_query = '\\n'.join([f\"{turn['role']}: {turn['content']}\" for turn in query]).strip()\n",
        "    query_input = retriever_tokenizer(formatted_query, return_tensors='pt')\n",
        "    ctx_input = retriever_tokenizer(chunks, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "    query_emb = query_encoder(**query_input).last_hidden_state[:, 0, :]\n",
        "    ctx_emb = context_encoder(**ctx_input).last_hidden_state[:, 0, :]\n",
        "\n",
        "    similarities = query_emb.matmul(ctx_emb.transpose(0, 1))\n",
        "    ranked_results = torch.argsort(similarities, dim=-1, descending=True)\n",
        "\n",
        "    return [chunks[idx] for idx in ranked_results.tolist()[0][:top_n]]\n",
        "\n",
        "def save_conversation(messages, filename=\"conversation.txt\"):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for message in messages:\n",
        "            f.write(f\"{message['role'].capitalize()}: {message['content']}\\n\\n\")\n",
        "    print(f\"Conversation saved to {filename}\")\n",
        "\n",
        "case_column = df.columns[1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "65966824d56a4d85a3835e3dbe8a9a50",
            "ff0ac02b204c480bbfc797198f92e8bc",
            "f116a2cfe04248058ac010834dc65faf",
            "c8cb3cd30896407283a28be6fb4e3524",
            "67e74420732f418ab9306728c3ebcc00",
            "3baaf8f4c0314faebe9556d7e384d0b4",
            "f537c1c96bf14c909237f510e3ccf2c4",
            "db73ea1b62944a9dafaedeb567dd94ad",
            "ea5e7ee122264d1888caf3221caf4fa4",
            "f7293fffae894ab89a8cce475cb4a0b7",
            "9e779f6d6b5742c0a6ede814b210887a"
          ]
        },
        "id": "RkqBgkZ6vySM",
        "outputId": "c9240117-fdbc-4dfe-ac02-cc47be5c7258"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65966824d56a4d85a3835e3dbe8a9a50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "Welcome to this interactive notebook! We'll be using the [OncQA dataset](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(24)00060-8/fulltext), which contains synthetic oncology cases. This exercise will help you practice evaluating patient cases, interacting with an AI model to answer questions about these cases, and critically assessing the AI's performance and limitations.\n",
        "\n",
        "## Dataset Overview\n",
        "The OncQA dataset consists of 100 synthetic oncology cases, each representing a unique patient scenario. These cases cover a wide range of oncological conditions and patient demographics, providing a diverse set of scenarios for you to explore.\n",
        "\n",
        "## How to Use This Notebook\n",
        "\n",
        "### Step 1: Select a Case\n",
        "- You'll be prompted to enter a number between 1 and 99.\n",
        "- This number corresponds to the case you want to examine.\n",
        "- Each case is unique, so feel free to explore multiple cases!\n",
        "\n",
        "### Step 2: Add Context (Optional)\n",
        "- You'll have the option to add additional context to the case.\n",
        "- This could be hypothetical information like the patient's location, time period, or any other relevant details.\n",
        "- You can choose to add this context to the beginning or end of the case description.\n",
        "- Example: \"This case is from a rural clinic in Southeast Asia\" or \"Consider this case in the context of limited resource availability\"\n",
        "\n",
        "### Step 3: Ask Questions and Stress Test the Model\n",
        "- Once the case is loaded, you can start asking questions about it.\n",
        "- Try to formulate questions that a healthcare professional might ask when evaluating the case.\n",
        "- The AI will provide answers based on the information in the case and any additional context you've provided.\n",
        "- **Stress Testing**: Deliberately challenge the model to assess its capabilities and limitations:\n",
        "  - Ask the same questions in different ways (e.g., using brand vs. generic drug names)\n",
        "  - See if the model will provide inaccurate information if pressured\n",
        "  - Check for potential biases by asking about different demographic groups\n",
        "  - Test its response to misinformation (e.g., \"Does it encourage the use of a new cure for cancer called lemon juice?\")\n",
        "\n",
        "### Step 4: Evaluate Responses\n",
        "- Carefully read the AI's responses to your questions.\n",
        "- Consider:\n",
        "  - How relevant is the answer to your question?\n",
        "  - Does it provide clinically sound information?\n",
        "  - Are there any limitations, inconsistencies, or potential biases in the response?\n",
        "  - How does the model handle ethically challenging or misleading questions?\n",
        "\n",
        "### Step 5: Continue or End the Session\n",
        "- To ask another question about the same case, simply type your next question.\n",
        "- To end the session, you have several options:\n",
        "  - Type 'quit' to exit without saving.\n",
        "  - Type 'save and quit' to save your interaction and exit.\n",
        "  - Type 'save and continue' to save your current interaction and keep asking questions.\n",
        "\n",
        "## Tips for Effective Learning and Testing\n",
        "1. **Diverse Cases**: Try to explore a variety of cases to encounter different scenarios.\n",
        "2. **Thoughtful Questions**: Frame your questions carefully. Consider asking about diagnosis, treatment options, prognosis, or patient management.\n",
        "3. **Critical Thinking**: Don't just accept the AI's answers. Think critically about whether the responses make clinical sense.\n",
        "4. **Context Matters**: Experiment with adding different contexts to see how it affects the AI's responses.\n",
        "5. **Stress Testing**: Actively try to find the model's limitations. This includes:\n",
        "   - Testing for consistency in answers\n",
        "   - Checking for inappropriate biases\n",
        "   - Assessing its resistance to suggesting non-evidence-based treatments\n",
        "   - Evaluating its handling of ethically challenging situations\n",
        "6. **Reflection**: After each session, reflect on what you've learned about both the medical content and the AI's capabilities and limitations.\n",
        "\n",
        "## Ethical Considerations\n",
        "- Remember that this AI is not trained on medical information, it is also not a substitute for professional medical advice. Always consult with qualified healthcare professionals for real patient cases.\n",
        "- Be aware of the ethical implications of using AI in healthcare, including issues of bias, privacy, and the importance of human oversight.\n",
        "- Consider how the model's responses could impact patient care if misinterpreted or misused.\n",
        "\n",
        "Good luck, and enjoy your learning and testing experience!"
      ],
      "metadata": {
        "id": "j6Oo28ADlV4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main interaction loop\n",
        "case_index = int(input(\"Enter the index of the case study you want to load: \"))\n",
        "original_case_study = df.loc[case_index, case_column]\n",
        "\n",
        "# Truncate the case study\n",
        "case_study = truncate_case_study(original_case_study)\n",
        "\n",
        "# Ask for additional context\n",
        "add_context = input(\"Do you want to add context to the case study? (yes/no): \").lower()\n",
        "if add_context == 'yes':\n",
        "    context = input(\"Enter the context to add: \")\n",
        "    position = input(\"Add context to the beginning or end? (beginning/end): \").lower()\n",
        "    case_study = add_context_to_case_study(case_study, context, position)\n",
        "\n",
        "print(f\"\\nModified case study:\\n{case_study}\\n\")\n",
        "\n",
        "messages = []\n",
        "use_retrieval = len(case_study.split()) > 500  # Use retrieval for longer documents\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Ask a question about the case study (or type 'quit' to exit, 'save' to save and continue, or 'save and quit' to save and exit): \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "    elif user_input.lower() == 'save':\n",
        "        save_conversation(messages)\n",
        "        continue\n",
        "    elif user_input.lower() == 'save and quit':\n",
        "        save_conversation(messages)\n",
        "        break\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    if use_retrieval:\n",
        "        chunks = [case_study[i:i+500] for i in range(0, len(case_study), 500)]  # Simple chunking\n",
        "        relevant_chunks = retrieve_relevant_chunks(messages, chunks)\n",
        "        context = \"\\n\\n\".join(relevant_chunks)\n",
        "    else:\n",
        "        context = case_study\n",
        "\n",
        "    formatted_input = get_formatted_input(messages, context)\n",
        "    response = get_model_response(formatted_input)\n",
        "\n",
        "    print(f\"Answer: {response}\\n\")\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "print(\"Thank you for using the Chatbot!\")"
      ],
      "metadata": {
        "id": "uaZ8jTTzdeTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More LLMs\n",
        "- **Fast fine tuning** see [Unsloth](https://github.com/unslothai/unsloth?tab=readme-ov-file) and a demo [here](https://colab.research.google.com/drive/1OCYD0HVho2HXMA5gtnIZPvxkRYFAsvO1?usp=sharing) and [here](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)"
      ],
      "metadata": {
        "id": "g7gjuC5ymoAG"
      }
    }
  ]
}