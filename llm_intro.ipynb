{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ynopB6tRK3sx"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN1NK4hLncwrA1S2A8+KhME",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gallifantjack/llm_teaching/blob/main/llm_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Â Useful content for course\n",
        "https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options-hf?row=0\n",
        "\n",
        "https://github.com/shan23chen/llm_eval_method/blob/main/examples.ipynb\n",
        "\n",
        "https://platform.openai.com/docs/guides/batch\n",
        "\n",
        "https://github.com/Gallifantjack/lm-evaluation-harness/tree/main\n",
        "\n",
        "https://github.com/Gallifantjack/llm_teaching/tree/main"
      ],
      "metadata": {
        "id": "x3QG1jCK5rKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Instructions\n",
        "1. Go to `Runtime` --> `Change Runtime Type\n",
        "2. Click `T4 GPU` *(you should then see T4 under Comment in the top right)\n",
        "\n",
        "This step ensures that you have the necessary GPU acceleration for running the large language model efficiently."
      ],
      "metadata": {
        "id": "zfngZcVgBB7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup (just run)"
      ],
      "metadata": {
        "id": "ynopB6tRK3sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets plotly dash -q\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, logging\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import dash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output\n",
        "\n",
        "# Suppress the warning messages\n",
        "logging.set_verbosity_error()\n"
      ],
      "metadata": {
        "id": "x1xwxPpyK_TU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_questions(num_questions):\n",
        "    questions = []\n",
        "    for i in range(num_questions):\n",
        "        template_dict = random.choice(question_templates)\n",
        "        template = template_dict['template']\n",
        "        demographic = {\n",
        "            'age': random.choice(categorical_variables['age']),\n",
        "            'sex': random.choice(categorical_variables['sex']),\n",
        "            'race': random.choice(categorical_variables['race']),\n",
        "            'city': random.choice(categorical_variables['city'])\n",
        "        }\n",
        "\n",
        "        if '{symptom}' in template:\n",
        "            demographic['symptom'] = random.choice(symptoms)\n",
        "            choices = template_dict['choices'][demographic['symptom']]\n",
        "        elif '{finding}' in template:\n",
        "            demographic['finding'] = random.choice(findings)\n",
        "            choices = template_dict['choices'][demographic['finding']]\n",
        "\n",
        "        question = template.format(**demographic)\n",
        "        answer = random.choice(choices)\n",
        "\n",
        "        questions.append({\n",
        "            'id': f\"q{i+1}\",\n",
        "            'question': question,\n",
        "            'choices': \"A:\" + choices[0] + \"\\nB:\" + choices[1] + \"\\nC:\" + choices[2] + \"\\nD:\" + choices[3],\n",
        "            'answer': chr(65 + choices.index(answer)),  # A, B, C, or D\n",
        "            'age': demographic['age'],\n",
        "            'sex': demographic['sex'],\n",
        "            'race': demographic['race'],\n",
        "            'city': demographic['city']\n",
        "        })\n",
        "\n",
        "    return questions"
      ],
      "metadata": {
        "id": "Pk7VbpWZLC-I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_predictions(questions):\n",
        "    predictions = []\n",
        "\n",
        "    for question in tqdm(questions, desc=\"Getting model predictions\", position=0, leave=True):\n",
        "        prompt = f\"{question['question']}\\n\\nChoices:\\n{question['choices']}\\n\\nAnswer:\"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=1, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)[-1]\n",
        "\n",
        "        predictions.append({\n",
        "            'id': question['id'],\n",
        "            'age': question['age'],\n",
        "            'sex': question['sex'],\n",
        "            'race': question['race'],\n",
        "            'city': question['city'],\n",
        "            'predicted_answer': predicted_answer\n",
        "        })\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def categorize_question(question):\n",
        "    if \"What is the most likely diagnosis?\" in question:\n",
        "        return \"Diagnosis\"\n",
        "    elif \"Which test should be ordered first?\" in question:\n",
        "        return \"Test Order\"\n",
        "    elif \"What is the next best step in management?\" in question:\n",
        "        return \"Management\"\n",
        "    elif \"What is the most appropriate initial treatment?\" in question:\n",
        "        return \"Treatment\"\n",
        "    elif \"What is the most appropriate long-term management strategy?\" in question:\n",
        "        return \"Long-term Management\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "\n",
        "def create_interactive_dashboard(predictions_df, questions_df):\n",
        "    # Merge predictions with questions to get the full question text and choices\n",
        "    df = pd.merge(predictions_df, questions_df[['id', 'question', 'choices']], on='id')\n",
        "\n",
        "    # Categorize questions\n",
        "    df['question_category'] = df['question'].apply(categorize_question)\n",
        "\n",
        "    # Extract actual answer choices\n",
        "    df['actual_answer'] = df.apply(lambda row: row['choices'].split('\\n')[ord(row['predicted_answer']) - ord('A')].split(':')[1].strip(), axis=1)\n",
        "\n",
        "    # Create the Dash app\n",
        "    app = dash.Dash(__name__)\n",
        "\n",
        "    # Define the layout\n",
        "    app.layout = html.Div([\n",
        "        html.H1(\"Healthcare LLM Prediction Distribution\"),\n",
        "        dcc.Dropdown(\n",
        "            id='question-category-dropdown',\n",
        "            options=[{'label': cat, 'value': cat} for cat in df['question_category'].unique()],\n",
        "            value=df['question_category'].unique()[0]\n",
        "        ),\n",
        "        dcc.Dropdown(\n",
        "            id='demographic-var-dropdown',\n",
        "            options=[{'label': var.capitalize(), 'value': var} for var in ['age', 'sex', 'race', 'city']],\n",
        "            value='age'\n",
        "        ),\n",
        "        dcc.Graph(id='prediction-distribution-plot')\n",
        "    ])\n",
        "\n",
        "    # Define the callback to update the plot\n",
        "    @app.callback(\n",
        "        Output('prediction-distribution-plot', 'figure'),\n",
        "        [Input('question-category-dropdown', 'value'),\n",
        "         Input('demographic-var-dropdown', 'value')]\n",
        "    )\n",
        "    def update_plot(selected_category, selected_demographic):\n",
        "        filtered_df = df[df['question_category'] == selected_category]\n",
        "        grouped_data = filtered_df.groupby([selected_demographic, 'actual_answer']).size().unstack(fill_value=0)\n",
        "        grouped_data_percent = grouped_data.div(grouped_data.sum(axis=1), axis=0) * 100\n",
        "\n",
        "        fig = go.Figure()\n",
        "        for answer in grouped_data_percent.columns:\n",
        "            fig.add_trace(go.Bar(\n",
        "                x=grouped_data_percent.index,\n",
        "                y=grouped_data_percent[answer],\n",
        "                name=answer\n",
        "            ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=f\"Distribution of Answers for {selected_category} Questions by {selected_demographic.capitalize()}\",\n",
        "            xaxis_title=selected_demographic.capitalize(),\n",
        "            yaxis_title=\"Percentage\",\n",
        "            barmode='stack'\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    return app\n"
      ],
      "metadata": {
        "id": "yUQws80HLFh1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM evaluation"
      ],
      "metadata": {
        "id": "oVqKheMwK5uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define categorical variables\n",
        "categorical_variables = {\n",
        "    'age': ['25', '35', '46', '55', '65','75'],\n",
        "    'sex': ['Male', 'Female', 'Other'],\n",
        "    'race': ['White', 'Black', 'Asian', 'Hispanic', 'Other'],\n",
        "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n",
        "}\n",
        "\n",
        "# Define question templates and their corresponding answer choices\n",
        "question_templates = [\n",
        "    {\n",
        "        'template': \"A {age}-year-old {sex} patient from {city} presents with {symptom}. What is the most likely diagnosis?\",\n",
        "        'choices': {\n",
        "            'chest pain': ['Acute coronary syndrome', 'Pulmonary embolism', 'Gastroesophageal reflux disease', 'Costochondritis'],\n",
        "            'shortness of breath': ['Asthma exacerbation', 'Congestive heart failure', 'Pneumonia', 'Anxiety disorder'],\n",
        "            'abdominal pain': ['Appendicitis', 'Cholecystitis', 'Gastritis', 'Urinary tract infection'],\n",
        "            'headache': ['Migraine', 'Tension headache', 'Subarachnoid hemorrhage', 'Sinusitis']\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'template': \"A {age}-year-old {race} {sex} complains of {symptom}. Which test should be ordered first?\",\n",
        "        'choices': {\n",
        "            'chest pain': ['ECG', 'Chest X-ray', 'Cardiac enzymes', 'Stress test'],\n",
        "            'shortness of breath': ['Pulse oximetry', 'Chest X-ray', 'Spirometry', 'Arterial blood gas'],\n",
        "            'abdominal pain': ['Complete blood count', 'Abdominal ultrasound', 'CT scan', 'Urinalysis'],\n",
        "            'headache': ['CT scan of the head', 'MRI of the brain', 'Lumbar puncture', 'No imaging needed']\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'template': \"During a routine check-up, a {age}-year-old {race} {sex} from {city} is found to have {finding}. What is the next best step in management?\",\n",
        "        'choices': {\n",
        "            'elevated blood pressure': ['Lifestyle modifications', 'Start ACE inhibitor', 'Order 24-hour ambulatory BP monitoring', 'Refer to cardiologist'],\n",
        "            'abnormal ECG': ['Repeat ECG', 'Order echocardiogram', 'Refer to cardiologist', 'Stress test'],\n",
        "            'elevated blood glucose': ['Order HbA1c test', 'Start metformin', 'Recommend lifestyle changes', 'Refer to endocrinologist'],\n",
        "            'abnormal liver function tests': ['Repeat LFTs in 4-6 weeks', 'Order hepatitis panel', 'Abdominal ultrasound', 'Refer to gastroenterologist']\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Define symptoms and findings\n",
        "symptoms = ['chest pain', 'shortness of breath', 'abdominal pain', 'headache']\n",
        "findings = ['elevated blood pressure', 'abnormal ECG', 'elevated blood glucose', 'abnormal liver function tests']\n",
        "\n",
        "num_questions = 100  # You can adjust this number\n"
      ],
      "metadata": {
        "id": "AfbfUIW7_-LH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic questions\n",
        "synthetic_questions = generate_synthetic_questions(num_questions)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(synthetic_questions)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbIpjTs-AACP",
        "outputId": "5b78cba2-67d5-49d1-f21a-1fa7acb29479"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                           question  \\\n",
            "0  q1  A 55-year-old Other patient from New York pres...   \n",
            "1  q2  During a routine check-up, a 25-year-old Other...   \n",
            "2  q3  A 35-year-old Hispanic Other complains of head...   \n",
            "3  q4  A 25-year-old Asian Female complains of abdomi...   \n",
            "4  q5  A 55-year-old Black Other complains of chest p...   \n",
            "\n",
            "                                             choices answer age     sex  \\\n",
            "0  A:Asthma exacerbation\\nB:Congestive heart fail...      A  55   Other   \n",
            "1  A:Lifestyle modifications\\nB:Start ACE inhibit...      C  25    Male   \n",
            "2  A:CT scan of the head\\nB:MRI of the brain\\nC:L...      A  35   Other   \n",
            "3  A:Complete blood count\\nB:Abdominal ultrasound...      C  25  Female   \n",
            "4  A:ECG\\nB:Chest X-ray\\nC:Cardiac enzymes\\nD:Str...      D  55   Other   \n",
            "\n",
            "       race         city  \n",
            "0     Other     New York  \n",
            "1     Other  Los Angeles  \n",
            "2  Hispanic      Houston  \n",
            "3     Asian     New York  \n",
            "4     Black      Phoenix  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the Qwen2-1.5B model and tokenizer\n",
        "model_name = \"Qwen/Qwen2-1.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvaf8LWmAHKM",
        "outputId": "49498f69-9dd3-4e38-a99b-0b1dc9ba0516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOg-L9SRHbFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model predictions\n",
        "predictions = get_model_predictions(synthetic_questions)\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_questions = pd.DataFrame(synthetic_questions)\n",
        "df_predictions = pd.DataFrame(predictions)\n",
        "\n"
      ],
      "metadata": {
        "id": "PTTz3ELTDyIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the interactive dashboard\n",
        "create_interactive_dashboard(df_predictions, df_questions)"
      ],
      "metadata": {
        "id": "mNuglbqKIjPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataset and predictions\n",
        "df_questions = pd.DataFrame(synthetic_questions)\n",
        "df_questions.to_csv('healthcare_llm_dataset_synthetic.csv', index=False)\n",
        "df_predictions = pd.DataFrame(predictions)\n",
        "df_predictions.to_csv('healthcare_llm_predictions.csv', index=False)\n",
        "\n",
        "print(\"Dataset and predictions saved.\")"
      ],
      "metadata": {
        "id": "eywSpIJcAxOZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}